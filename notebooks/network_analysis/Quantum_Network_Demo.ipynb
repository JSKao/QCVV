{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Network Frontiers: Multi-Scale Problem Analysis\n",
    "\n",
    "## Core Challenges in Quantum Networks\n",
    "- **Scalability**: Quantum memory limitations require rapid control decisions\n",
    "- **Network Architecture**: Cross-layer protocol stack design and optimization\n",
    "- **Security Verification**: Device-independent certification protocols\n",
    "\n",
    "## Problem Framework\n",
    "1. **Decoherence Analysis** ‚Üí Network topology optimization\n",
    "2. **Traffic Prediction** ‚Üí Adaptive routing strategies\n",
    "3. **Security Certification** ‚Üí End-to-end trust establishment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup and tool implementation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Environment initialized\")\n",
    "print(\"Loading quantum network analysis tools...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Decoherence Analysis Tool\n",
    "class QuantumNetworkDecoherence:\n",
    "    \"\"\"Network-scale decoherence modeling with spatial correlations\"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes: int, node_spacing_km: float):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.node_spacing = node_spacing_km\n",
    "        self.node_positions = np.arange(num_nodes) * node_spacing_km\n",
    "        self.correlation_length = 200  # km\n",
    "        self.correlation_strength = 0.1\n",
    "        self.correlation_matrix = self._build_correlation_matrix()\n",
    "        \n",
    "    def _build_correlation_matrix(self):\n",
    "        \"\"\"Spatial correlation matrix based on exponential decay\"\"\"\n",
    "        matrix = np.eye(self.num_nodes)\n",
    "        for i in range(self.num_nodes):\n",
    "            for j in range(i + 1, self.num_nodes):\n",
    "                distance = abs(self.node_positions[i] - self.node_positions[j])\n",
    "                correlation = np.exp(-distance / self.correlation_length)\n",
    "                matrix[i, j] = correlation * self.correlation_strength\n",
    "                matrix[j, i] = matrix[i, j]\n",
    "        return matrix\n",
    "    \n",
    "    def simulate_decoherence(self, time_points: np.ndarray, decay_rate: float = 0.05):\n",
    "        \"\"\"Simulate network-wide decoherence evolution\"\"\"\n",
    "        fidelities = {}\n",
    "        \n",
    "        for node_id in range(self.num_nodes):\n",
    "            # Effective decay includes correlation effects\n",
    "            effective_decay = decay_rate * (1 + 0.5 * np.sum(self.correlation_matrix[node_id, :]))\n",
    "            node_fidelity = np.exp(-effective_decay * time_points)\n",
    "            \n",
    "            # Add realistic noise\n",
    "            noise = 0.05 * np.random.normal(0, 1, len(time_points))\n",
    "            node_fidelity = np.clip(node_fidelity + noise, 0, 1)\n",
    "            \n",
    "            fidelities[node_id] = node_fidelity\n",
    "            \n",
    "        return {\n",
    "            'time_points': time_points,\n",
    "            'fidelities': fidelities,\n",
    "            'correlation_matrix': self.correlation_matrix\n",
    "        }\n",
    "    \n",
    "    def analyze_network_performance(self, simulation_results):\n",
    "        \"\"\"Extract performance metrics from simulation\"\"\"\n",
    "        fidelities = simulation_results['fidelities']\n",
    "        time_points = simulation_results['time_points']\n",
    "        \n",
    "        avg_fidelity = np.mean([fidelities[i] for i in range(self.num_nodes)], axis=0)\n",
    "        \n",
    "        # Fit exponential decay\n",
    "        valid_indices = avg_fidelity > 0.1\n",
    "        if np.sum(valid_indices) > 1:\n",
    "            log_fidelity = np.log(avg_fidelity[valid_indices])\n",
    "            valid_time = time_points[valid_indices]\n",
    "            decay_fit = np.polyfit(valid_time, log_fidelity, 1)\n",
    "            effective_decay_rate = -decay_fit[0]\n",
    "        else:\n",
    "            effective_decay_rate = 0.1\n",
    "            \n",
    "        coherence_time = 1 / effective_decay_rate if effective_decay_rate > 0 else np.inf\n",
    "        final_avg_fidelity = avg_fidelity[-1]\n",
    "        \n",
    "        return {\n",
    "            'effective_decay_rate': effective_decay_rate,\n",
    "            'coherence_time': coherence_time,\n",
    "            'final_avg_fidelity': final_avg_fidelity,\n",
    "            'avg_fidelity_evolution': avg_fidelity\n",
    "        }\n",
    "\n",
    "print(\"Decoherence analysis tool loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-contained Tool 2: Quantum Network Security Certification (Based on QCVV concepts)\n",
    "class QuantumNetworkSecurity:\n",
    "    \"\"\"Quantum network security certification tool\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.network_topology = {}\n",
    "        self.connections = []\n",
    "        \n",
    "    def add_node(self, node_id: str, capabilities: Dict):\n",
    "        \"\"\"Add network node\"\"\"\n",
    "        self.network_topology[node_id] = capabilities\n",
    "        \n",
    "    def add_connection(self, node1: str, node2: str):\n",
    "        \"\"\"Add node connection\"\"\"\n",
    "        if (node1, node2) not in self.connections and (node2, node1) not in self.connections:\n",
    "            self.connections.append((node1, node2))\n",
    "    \n",
    "    def simulate_bell_test(self, node1_qubits: int, node2_qubits: int):\n",
    "        \"\"\"Simulate Bell inequality test\"\"\"\n",
    "        # Select test type based on number of qubits\n",
    "        if min(node1_qubits, node2_qubits) >= 3:\n",
    "            # Mermin test: stronger violation\n",
    "            quantum_bound = 4.0\n",
    "            classical_bound = 2.0\n",
    "            test_type = \"Mermin\"\n",
    "        else:\n",
    "            # CHSH test: standard 2-qubit\n",
    "            quantum_bound = 2.828\n",
    "            classical_bound = 2.0\n",
    "            test_type = \"CHSH\"\n",
    "            \n",
    "        # Simulate realistic test results (with imperfections)\n",
    "        noise_factor = 0.95 + 0.1 * np.random.random()  # 95-105% of theoretical value\n",
    "        measured_value = quantum_bound * noise_factor\n",
    "        \n",
    "        violation = measured_value > classical_bound\n",
    "        security_parameter = max(0, measured_value - classical_bound)\n",
    "        \n",
    "        return {\n",
    "            'test_type': test_type,\n",
    "            'measured_value': measured_value,\n",
    "            'classical_bound': classical_bound,\n",
    "            'quantum_bound': quantum_bound,\n",
    "            'violates_classical': violation,\n",
    "            'security_parameter': security_parameter\n",
    "        }\n",
    "    \n",
    "    def run_network_certification(self):\n",
    "        \"\"\"Execute network-level security certification\"\"\"\n",
    "        certification_results = {}\n",
    "        total_connections = len(self.connections)\n",
    "        passed_tests = 0\n",
    "        security_parameters = []\n",
    "        \n",
    "        for node1, node2 in self.connections:\n",
    "            node1_qubits = self.network_topology[node1]['qubits']\n",
    "            node2_qubits = self.network_topology[node2]['qubits']\n",
    "            \n",
    "            test_result = self.simulate_bell_test(node1_qubits, node2_qubits)\n",
    "            certification_results[(node1, node2)] = test_result\n",
    "            \n",
    "            if test_result['violates_classical']:\n",
    "                passed_tests += 1\n",
    "                security_parameters.append(test_result['security_parameter'])\n",
    "                \n",
    "        # Calculate network-level metrics\n",
    "        pass_rate = passed_tests / total_connections if total_connections > 0 else 0\n",
    "        avg_security = np.mean(security_parameters) if security_parameters else 0\n",
    "        min_security = np.min(security_parameters) if security_parameters else 0\n",
    "        network_trust_score = pass_rate * min_security\n",
    "        \n",
    "        return {\n",
    "            'individual_results': certification_results,\n",
    "            'network_certified': (passed_tests == total_connections and passed_tests > 0),\n",
    "            'pass_rate': pass_rate,\n",
    "            'average_security_parameter': avg_security,\n",
    "            'min_security_parameter': min_security,\n",
    "            'network_trust_score': network_trust_score,\n",
    "            'total_connections': total_connections,\n",
    "            'passed_connections': passed_tests\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Security certification tool loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Traffic Analysis Tool\n",
    "class QuantumTrafficAnalyzer:\n",
    "    \"\"\"Traffic pattern analysis and congestion prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, cities: List[str]):\n",
    "        self.cities = cities\n",
    "        self.num_cities = len(cities)\n",
    "        \n",
    "    def generate_realistic_traffic(self, time_hours: int = 24, samples_per_hour: int = 4):\n",
    "        \"\"\"Generate time-varying traffic demand patterns\"\"\"\n",
    "        total_samples = time_hours * samples_per_hour\n",
    "        time_grid = np.linspace(0, time_hours, total_samples)\n",
    "        \n",
    "        traffic_data = {}\n",
    "        \n",
    "        for i, city1 in enumerate(self.cities):\n",
    "            for j, city2 in enumerate(self.cities):\n",
    "                if i < j:\n",
    "                    pair_key = f\"{city1}-{city2}\"\n",
    "                    \n",
    "                    # Business hours pattern (6:00-22:00 peak)\n",
    "                    business_pattern = 0.3 + 0.6 * (np.sin((time_grid - 6) * np.pi / 8) > 0) * np.sin((time_grid - 6) * np.pi / 8)\n",
    "                    \n",
    "                    # Application-specific patterns\n",
    "                    if \"Amsterdam\" in pair_key:\n",
    "                        app_factor = 0.8 + 0.2 * np.sin(time_grid * 2 * np.pi / 12)\n",
    "                    elif \"Berlin\" in pair_key:\n",
    "                        app_factor = 0.5 + 0.3 * np.sin(time_grid * 2 * np.pi / 6) + 0.2 * (np.random.random(len(time_grid)) > 0.8)\n",
    "                    else:\n",
    "                        app_factor = 0.6 + 0.3 * np.sin(time_grid * 2 * np.pi / 16)\n",
    "                    \n",
    "                    total_demand = business_pattern * app_factor\n",
    "                    noise = 0.1 * np.random.normal(0, 1, len(time_grid))\n",
    "                    final_demand = np.clip(total_demand + noise, 0.05, 1.0)\n",
    "                    \n",
    "                    traffic_data[pair_key] = {\n",
    "                        'demand': final_demand,\n",
    "                        'time_grid': time_grid,\n",
    "                        'avg_demand': np.mean(final_demand),\n",
    "                        'peak_demand': np.max(final_demand),\n",
    "                        'variability': np.std(final_demand)\n",
    "                    }\n",
    "        \n",
    "        return traffic_data\n",
    "    \n",
    "    def analyze_traffic_patterns(self, traffic_data):\n",
    "        \"\"\"Extract routing priorities from traffic analysis\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        for pair_key, data in traffic_data.items():\n",
    "            demand = data['demand']\n",
    "            time_grid = data['time_grid']\n",
    "            \n",
    "            peak_threshold = np.percentile(demand, 80)\n",
    "            peak_times = time_grid[demand > peak_threshold]\n",
    "            \n",
    "            stability_score = 1 / (1 + data['variability'])\n",
    "            priority_score = data['avg_demand'] * stability_score\n",
    "            \n",
    "            analysis[pair_key] = {\n",
    "                'peak_times': peak_times,\n",
    "                'stability_score': stability_score,\n",
    "                'priority_score': priority_score,\n",
    "                'routing_recommendation': 'high' if priority_score > 0.5 else 'medium' if priority_score > 0.3 else 'low'\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def predict_network_congestion(self, traffic_data, future_hours: int = 4):\n",
    "        \"\"\"Predict network congestion based on historical patterns\"\"\"\n",
    "        total_demand_timeline = np.zeros(len(list(traffic_data.values())[0]['time_grid']))\n",
    "        \n",
    "        for data in traffic_data.values():\n",
    "            total_demand_timeline += data['demand']\n",
    "        \n",
    "        current_time = list(traffic_data.values())[0]['time_grid'][-1]\n",
    "        future_time = np.linspace(current_time, current_time + future_hours, future_hours * 4)\n",
    "        \n",
    "        # Pattern-based prediction\n",
    "        recent_pattern = total_demand_timeline[-20:]\n",
    "        predicted_pattern = np.tile(recent_pattern, (future_hours // 5) + 1)[:len(future_time)]\n",
    "        \n",
    "        congestion_threshold = np.mean(total_demand_timeline) + 1.5 * np.std(total_demand_timeline)\n",
    "        congestion_times = future_time[predicted_pattern > congestion_threshold]\n",
    "        \n",
    "        return {\n",
    "            'future_time': future_time,\n",
    "            'predicted_demand': predicted_pattern,\n",
    "            'congestion_threshold': congestion_threshold,\n",
    "            'congestion_times': congestion_times,\n",
    "            'congestion_risk': len(congestion_times) / len(future_time)\n",
    "        }\n",
    "\n",
    "print(\"Traffic analysis tool loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem 1: Network Topology Impact on Decoherence\n",
    "\n",
    "Quantum networks face fundamental limitations from decoherence propagation. Different network topologies exhibit varying resilience to environmental noise and spatial correlations. This analysis compares linear chain networks versus star-hub configurations using realistic European city distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1.1: Establish European Quantum Network Scenario\n",
    "\n",
    "print(\"üîç Problem 1: Quantum Network Topology Noise Resistance Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define four major European city quantum nodes\n",
    "cities = [\"Amsterdam\", \"Brussels\", \"Paris\", \"Berlin\"]\n",
    "num_nodes = len(cities)\n",
    "\n",
    "# Real distances (kilometers)\n",
    "city_distances = {\n",
    "    \"Amsterdam\": 0,\n",
    "    \"Brussels\": 173,      # Amsterdam -> Brussels\n",
    "    \"Paris\": 431,         # Amsterdam -> Paris  \n",
    "    \"Berlin\": 576          # Amsterdam -> Berlin\n",
    "}\n",
    "\n",
    "print(f\"üìç Quantum network nodes: {cities}\")\n",
    "print(f\"üó∫Ô∏è  Node distances (relative to Amsterdam):\")\n",
    "for city, distance in city_distances.items():\n",
    "    print(f\"   {city}: {distance} km\")\n",
    "\n",
    "# Simulation parameters\n",
    "simulation_time = np.linspace(0, 3.0, 150)  # 3-second evolution, 150 time points\n",
    "base_decay_rate = 0.08  # Base decay rate (corresponds to ~12.5s coherence time)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Simulation settings:\")\n",
    "print(f\"   Evolution time: {simulation_time[-1]} seconds\")\n",
    "print(f\"   Time sampling: {len(simulation_time)} points\")\n",
    "print(f\"   Base decay rate: {base_decay_rate} s‚Åª¬π\")\n",
    "print(f\"   Expected coherence time: ~{1/base_decay_rate:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1.2: Linear Network Topology Analysis\n",
    "\n",
    "print(\"\\nüîó Building linear network topology (chain connection)\")\n",
    "print(\"   Amsterdam ‚Üí Brussels ‚Üí Paris ‚Üí Berlin\")\n",
    "\n",
    "# Linear topology: nodes arranged in geographical order\n",
    "linear_positions = [city_distances[city] for city in cities]\n",
    "linear_network = QuantumNetworkDecoherence(num_nodes, 1)  # spacing=1 because we manually set positions\n",
    "linear_network.node_positions = np.array(linear_positions)\n",
    "linear_network.correlation_matrix = linear_network._build_correlation_matrix()\n",
    "\n",
    "print(f\"\\nüìä Linear network characteristics:\")\n",
    "print(f\"   Node positions: {linear_network.node_positions} km\")\n",
    "print(f\"   Maximum distance: {np.max(linear_network.node_positions) - np.min(linear_network.node_positions)} km\")\n",
    "print(f\"   End-to-end correlation: {linear_network.correlation_matrix[0, -1]:.4f}\")\n",
    "\n",
    "# Execute linear network decoherence simulation\n",
    "print(\"\\n‚è≥ Running linear network decoherence simulation...\")\n",
    "linear_results = linear_network.simulate_decoherence(simulation_time, base_decay_rate)\n",
    "linear_analysis = linear_network.analyze_network_performance(linear_results)\n",
    "\n",
    "print(f\"‚úÖ Linear network simulation complete:\")\n",
    "print(f\"   Effective decay rate: {linear_analysis['effective_decay_rate']:.4f} s‚Åª¬π\")\n",
    "print(f\"   Network coherence time: {linear_analysis['coherence_time']:.2f} s\")\n",
    "print(f\"   Final fidelity: {linear_analysis['final_avg_fidelity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1.3: Star Network Topology Analysis\n",
    "\n",
    "print(\"\\\\n‚≠ê Building star network topology (Amsterdam as central hub)\")\n",
    "print(\"   Brussels ‚Üê Amsterdam ‚Üí Paris\")\n",
    "print(\"               ‚Üì\")\n",
    "print(\"            Berlin\")\n",
    "\n",
    "# Star topology: Amsterdam as center, other nodes equidistant\n",
    "# Use average distance for fair comparison\n",
    "avg_distance = np.mean([city_distances[city] for city in cities[1:]])\n",
    "star_positions = [0, avg_distance, avg_distance, avg_distance]  # Amsterdam at center\n",
    "\n",
    "star_network = QuantumNetworkDecoherence(num_nodes, 1)\n",
    "star_network.node_positions = np.array(star_positions)\n",
    "star_network.correlation_matrix = star_network._build_correlation_matrix()\n",
    "\n",
    "print(f\"\\\\nüìä Star network characteristics:\")\n",
    "print(f\"   Node positions: {star_network.node_positions} km\")\n",
    "print(f\"   Average distance: {avg_distance:.0f} km\")\n",
    "print(f\"   Center-edge correlation: {star_network.correlation_matrix[0, 1]:.4f}\")\n",
    "print(f\"   Edge-to-edge correlation: {star_network.correlation_matrix[1, 2]:.4f}\")\n",
    "\n",
    "# Execute star network decoherence simulation\n",
    "print(\"\\\\n‚è≥ Running star network decoherence simulation...\")\n",
    "star_results = star_network.simulate_decoherence(simulation_time, base_decay_rate)\n",
    "star_analysis = star_network.analyze_network_performance(star_results)\n",
    "\n",
    "print(f\"‚úÖ Star network simulation completed:\")\n",
    "print(f\"   Effective decay rate: {star_analysis['effective_decay_rate']:.4f} s‚Åª¬π\")\n",
    "print(f\"   Network coherence time: {star_analysis['coherence_time']:.2f} s\")\n",
    "print(f\"   Final fidelity: {star_analysis['final_avg_fidelity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1.4: Topology Comparison Analysis and Visualization\n",
    "\n",
    "print(\"\\\\nüìä Topology performance comparison analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Topology Type': ['Linear Network', 'Star Network'],\n",
    "    'Effective Decay Rate (s‚Åª¬π)': [linear_analysis['effective_decay_rate'], star_analysis['effective_decay_rate']],\n",
    "    'Coherence Time (s)': [linear_analysis['coherence_time'], star_analysis['coherence_time']],\n",
    "    'Final Fidelity': [linear_analysis['final_avg_fidelity'], star_analysis['final_avg_fidelity']]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\\\nüìã Topology comparison table:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Calculate improvement metrics\n",
    "coherence_improvement = (star_analysis['coherence_time'] - linear_analysis['coherence_time']) / linear_analysis['coherence_time'] * 100\n",
    "fidelity_improvement = (star_analysis['final_avg_fidelity'] - linear_analysis['final_avg_fidelity']) / linear_analysis['final_avg_fidelity'] * 100\n",
    "\n",
    "print(f\"\\\\nüîç Key findings:\")\n",
    "print(f\"   Star vs linear coherence time change: {coherence_improvement:+.1f}%\")\n",
    "print(f\"   Star vs linear fidelity change: {fidelity_improvement:+.1f}%\")\n",
    "\n",
    "# Determine best topology\n",
    "if star_analysis['coherence_time'] > linear_analysis['coherence_time']:\n",
    "    best_topology = \"star\"\n",
    "    best_analysis = star_analysis\n",
    "    print(f\"   üèÜ Best topology: Star network (longer coherence time)\")\n",
    "else:\n",
    "    best_topology = \"linear\"\n",
    "    best_analysis = linear_analysis\n",
    "    print(f\"   üèÜ Best topology: Linear network (longer coherence time)\")\n",
    "\n",
    "print(f\"\\\\n‚úÖ Topology analysis completed, selecting {best_topology} for subsequent security certification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1.5: Topology Performance Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Fidelity evolution comparison\n",
    "ax1.plot(simulation_time, linear_analysis['avg_fidelity_evolution'], \n",
    "         'b-', linewidth=2.5, label='Linear Network', alpha=0.8)\n",
    "ax1.plot(simulation_time, star_analysis['avg_fidelity_evolution'], \n",
    "         'r-', linewidth=2.5, label='Star Network', alpha=0.8)\n",
    "ax1.set_xlabel('Time (seconds)')\n",
    "ax1.set_ylabel('Average Fidelity')\n",
    "ax1.set_title('Quantum Network Fidelity Evolution Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# 2. Correlation matrix heatmap - Linear\n",
    "im1 = ax2.imshow(linear_network.correlation_matrix, cmap='Blues', vmin=0, vmax=0.1)\n",
    "ax2.set_title('Linear Network Spatial Correlation')\n",
    "ax2.set_xticks(range(num_nodes))\n",
    "ax2.set_yticks(range(num_nodes))\n",
    "ax2.set_xticklabels(cities, rotation=45)\n",
    "ax2.set_yticklabels(cities)\n",
    "plt.colorbar(im1, ax=ax2, fraction=0.046, pad=0.04)\n",
    "\n",
    "# 3. Correlation matrix heatmap - Star\n",
    "im2 = ax3.imshow(star_network.correlation_matrix, cmap='Reds', vmin=0, vmax=0.1)\n",
    "ax3.set_title('Star Network Spatial Correlation')\n",
    "ax3.set_xticks(range(num_nodes))\n",
    "ax3.set_yticks(range(num_nodes))\n",
    "ax3.set_xticklabels(cities, rotation=45)\n",
    "ax3.set_yticklabels(cities)\n",
    "plt.colorbar(im2, ax=ax3, fraction=0.046, pad=0.04)\n",
    "\n",
    "# 4. Performance metrics comparison bar chart\n",
    "metrics = ['Coherence Time (s)', 'Final Fidelity']\n",
    "linear_values = [linear_analysis['coherence_time'], linear_analysis['final_avg_fidelity']]\n",
    "star_values = [star_analysis['coherence_time'], star_analysis['final_avg_fidelity']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax4.bar(x - width/2, linear_values, width, label='Linear Network', color='skyblue', alpha=0.8)\n",
    "bars2 = ax4.bar(x + width/2, star_values, width, label='Star Network', color='lightcoral', alpha=0.8)\n",
    "\n",
    "ax4.set_xlabel('Performance Metrics')\n",
    "ax4.set_ylabel('Values')\n",
    "ax4.set_title('Topology Performance Comparison')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(metrics)\n",
    "ax4.legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Problem 1 completed! Key insights:\")\n",
    "print(f\"   ‚Ä¢ {best_topology} network performs better in European deployment scenario\")\n",
    "print(f\"   ‚Ä¢ Spatial correlation effects significantly impact decoherence evolution\")\n",
    "print(f\"   ‚Ä¢ Topology selection can provide {abs(coherence_improvement):.1f}% performance difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem 2: Adaptive Quantum Routing\n",
    "\n",
    "Quantum memory limitations require rapid routing decisions. This section develops intelligent routing strategies that integrate physical layer performance with dynamic traffic patterns to optimize resource allocation and minimize latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2.1: European Quantum Network Traffic Model\n",
    "\n",
    "print(\"üöÄ Problem 2: Smart Quantum Routing and Traffic Prediction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the best topology from Problem 1\n",
    "print(f\"üèÜ Using Problem 1 results: {best_topology} network topology\")\n",
    "\n",
    "# Initialize traffic analyzer\n",
    "european_cities = [\"Amsterdam\", \"Brussels\", \"Paris\", \"Berlin\"]\n",
    "traffic_analyzer = QuantumTrafficAnalyzer(european_cities)\n",
    "\n",
    "print(f\"üåç Quantum network nodes: {european_cities}\")\n",
    "print(f\"üìä Traffic analysis time range: 24 hours (6-hour resolution)\")\n",
    "\n",
    "# Generate 24-hour traffic patterns\n",
    "print(\"\\\\n‚è≥ Generating European quantum network traffic patterns...\")\n",
    "traffic_data = traffic_analyzer.generate_realistic_traffic(time_hours=24, samples_per_hour=4)\n",
    "\n",
    "print(f\"‚úÖ Traffic data generation completed:\")\n",
    "print(f\"   City pairs: {len(traffic_data)}\")\n",
    "print(f\"   Time sampling points: {len(list(traffic_data.values())[0]['time_grid'])}\")\n",
    "print(f\"   Highest average demand: {max([data['avg_demand'] for data in traffic_data.values()]):.3f}\")\n",
    "print(f\"   Highest peak demand: {max([data['peak_demand'] for data in traffic_data.values()]):.3f}\")\n",
    "\n",
    "# Display traffic characteristics for key city pairs\n",
    "key_pairs = [\"Amsterdam-Paris\", \"Amsterdam-Berlin\", \"Brussels-Paris\"]\n",
    "print(f\"\\\\nüìà Key connection traffic characteristics:\")\n",
    "for pair in key_pairs:\n",
    "    if pair in traffic_data:\n",
    "        data = traffic_data[pair]\n",
    "        print(f\"   {pair}:\")\n",
    "        print(f\"     Average demand: {data['avg_demand']:.3f}\")\n",
    "        print(f\"     Peak demand: {data['peak_demand']:.3f}\")\n",
    "        print(f\"     Variability: {data['variability']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2.2: Traffic Pattern Analysis and Routing Decisions\n",
    "\n",
    "print(\"\\\\nüîç Executing traffic pattern analysis...\")\n",
    "traffic_analysis = traffic_analyzer.analyze_traffic_patterns(traffic_data)\n",
    "\n",
    "print(\"üìä Routing priority analysis results:\")\n",
    "priority_data = []\n",
    "for pair_key, analysis in traffic_analysis.items():\n",
    "    priority_data.append({\n",
    "        'City Pair': pair_key,\n",
    "        'Stability Score': analysis['stability_score'],\n",
    "        'Priority Score': analysis['priority_score'],\n",
    "        'Routing Recommendation': analysis['routing_recommendation']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "priority_df = pd.DataFrame(priority_data)\n",
    "priority_df = priority_df.sort_values('Priority Score', ascending=False)\n",
    "print(\"\\\\nüìã Routing priority table:\")\n",
    "print(priority_df.round(3))\n",
    "\n",
    "# Identify high priority connections\n",
    "high_priority = priority_df[priority_df['Routing Recommendation'] == 'high']['City Pair'].tolist()\n",
    "medium_priority = priority_df[priority_df['Routing Recommendation'] == 'medium']['City Pair'].tolist()\n",
    "low_priority = priority_df[priority_df['Routing Recommendation'] == 'low']['City Pair'].tolist()\n",
    "\n",
    "print(f\"\\\\nüö® High priority connections ({len(high_priority)}): {high_priority}\")\n",
    "print(f\"‚ö†Ô∏è  Medium priority connections ({len(medium_priority)}): {medium_priority}\")\n",
    "print(f\"‚ÑπÔ∏è  Low priority connections ({len(low_priority)}): {low_priority}\")\n",
    "\n",
    "# Integrate physical layer information (from Problem 1)\n",
    "print(f\"\\\\nüîó Integrating physical layer performance ({best_topology} network):\")\n",
    "if best_topology == \"star\":\n",
    "    physical_performance = star_analysis\n",
    "    network_topology = star_network\n",
    "else:\n",
    "    physical_performance = linear_analysis\n",
    "    network_topology = linear_network\n",
    "\n",
    "print(f\"   Network coherence time: {physical_performance['coherence_time']:.2f} s\")\n",
    "print(f\"   Effective decay rate: {physical_performance['effective_decay_rate']:.4f} s‚Åª¬π\")\n",
    "print(f\"   Recommended routing decision time: < {physical_performance['coherence_time']/10:.1f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2.3: Congestion Prediction and Proactive Resource Allocation\n",
    "\n",
    "print(\"\\\\nüîÆ Executing network congestion prediction...\")\n",
    "congestion_forecast = traffic_analyzer.predict_network_congestion(traffic_data, future_hours=6)\n",
    "\n",
    "print(\"üìà Congestion prediction results:\")\n",
    "print(f\"   Prediction time range: {congestion_forecast['future_time'][-1] - congestion_forecast['future_time'][0]:.1f} hours\")\n",
    "print(f\"   Congestion threshold: {congestion_forecast['congestion_threshold']:.3f}\")\n",
    "print(f\"   Congestion risk: {congestion_forecast['congestion_risk']*100:.1f}%\")\n",
    "print(f\"   Expected congestion events: {len(congestion_forecast['congestion_times'])}\")\n",
    "\n",
    "if len(congestion_forecast['congestion_times']) > 0:\n",
    "    print(f\"   First congestion time: {congestion_forecast['congestion_times'][0]:.1f} hours later\")\n",
    "else:\n",
    "    print(\"   No congestion predicted\")\n",
    "\n",
    "# Build smart routing decision table\n",
    "print(\"\\\\nüß† Smart routing decision engine:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "routing_decisions = {}\n",
    "for pair_key in traffic_data.keys():\n",
    "    # Get traffic analysis results\n",
    "    analysis = traffic_analysis[pair_key]\n",
    "    \n",
    "    # Integrate physical layer and traffic layer information\n",
    "    physical_quality = physical_performance['final_avg_fidelity']\n",
    "    traffic_priority = analysis['priority_score']\n",
    "    stability = analysis['stability_score']\n",
    "    \n",
    "    # Calculate comprehensive routing score\n",
    "    routing_score = (physical_quality * 0.4 + traffic_priority * 0.4 + stability * 0.2)\n",
    "    \n",
    "    # Decision logic\n",
    "    if routing_score > 0.6:\n",
    "        decision = \"Immediate allocation\"\n",
    "        qos_level = \"premium\"\n",
    "    elif routing_score > 0.4:\n",
    "        decision = \"Conditional allocation\"\n",
    "        qos_level = \"standard\"\n",
    "    else:\n",
    "        decision = \"Delayed queue\"\n",
    "        qos_level = \"best-effort\"\n",
    "    \n",
    "    routing_decisions[pair_key] = {\n",
    "        'routing_score': routing_score,\n",
    "        'decision': decision,\n",
    "        'qos_level': qos_level,\n",
    "        'predicted_latency': 1.0 / routing_score  # Simplified latency model\n",
    "    }\n",
    "\n",
    "# Display routing decision results\n",
    "routing_summary = []\n",
    "for pair_key, decision in routing_decisions.items():\n",
    "    routing_summary.append({\n",
    "        'City Pair': pair_key,\n",
    "        'Routing Score': decision['routing_score'],\n",
    "        'Decision': decision['decision'],\n",
    "        'QoS Level': decision['qos_level'],\n",
    "        'Expected Latency': decision['predicted_latency']\n",
    "    })\n",
    "\n",
    "routing_df = pd.DataFrame(routing_summary)\n",
    "routing_df = routing_df.sort_values('Routing Score', ascending=False)\n",
    "print(\"\\\\nüìã Smart routing decision table:\")\n",
    "print(routing_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2.4: Smart Routing Results Visualization\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. 24-hour traffic evolution chart (top 3 main connections)\n",
    "main_connections = list(traffic_data.keys())[:3]\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "for i, pair_key in enumerate(main_connections):\n",
    "    data = traffic_data[pair_key]\n",
    "    ax1.plot(data['time_grid'], data['demand'], \n",
    "             color=colors[i], linewidth=2, label=pair_key, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Time (hours)')\n",
    "ax1.set_ylabel('Traffic Demand')\n",
    "ax1.set_title('European Quantum Network 24-Hour Traffic Evolution')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 24)\n",
    "\n",
    "# 2. Congestion prediction chart\n",
    "ax2.plot(congestion_forecast['future_time'], congestion_forecast['predicted_demand'], \n",
    "         'purple', linewidth=2.5, label='Predicted demand')\n",
    "ax2.axhline(y=congestion_forecast['congestion_threshold'], \n",
    "           color='red', linestyle='--', linewidth=2, label='Congestion threshold')\n",
    "\n",
    "if len(congestion_forecast['congestion_times']) > 0:\n",
    "    for congestion_time in congestion_forecast['congestion_times']:\n",
    "        ax2.axvline(x=congestion_time, color='red', alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Future time (hours)')\n",
    "ax2.set_ylabel('Predicted total demand')\n",
    "ax2.set_title('Network Congestion Prediction (Next 6 Hours)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Routing score distribution\n",
    "routing_scores = [decision['routing_score'] for decision in routing_decisions.values()]\n",
    "pair_names = [pair.replace('-', '\\\\n') for pair in routing_decisions.keys()]\n",
    "\n",
    "bars = ax3.bar(range(len(routing_scores)), routing_scores, \n",
    "               color=['green' if score > 0.6 else 'orange' if score > 0.4 else 'red' \n",
    "                      for score in routing_scores], alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('City Pairs')\n",
    "ax3.set_ylabel('Comprehensive Routing Score')\n",
    "ax3.set_title('Smart Routing Decision Scores')\n",
    "ax3.set_xticks(range(len(pair_names)))\n",
    "ax3.set_xticklabels(pair_names, rotation=45, ha='right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add score labels\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 4. QoS level statistics\n",
    "qos_levels = [decision['qos_level'] for decision in routing_decisions.values()]\n",
    "qos_counts = pd.Series(qos_levels).value_counts()\n",
    "\n",
    "colors_qos = {'premium': 'gold', 'standard': 'skyblue', 'best-effort': 'lightcoral'}\n",
    "ax4.pie(qos_counts.values, labels=qos_counts.index, autopct='%1.1f%%',\n",
    "        colors=[colors_qos.get(level, 'gray') for level in qos_counts.index])\n",
    "ax4.set_title('QoS Level Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate system performance metrics\n",
    "total_connections = len(routing_decisions)\n",
    "premium_count = sum(1 for d in routing_decisions.values() if d['qos_level'] == 'premium')\n",
    "avg_routing_score = np.mean(routing_scores)\n",
    "system_efficiency = premium_count / total_connections\n",
    "\n",
    "print(\"üéØ Problem 2 completed! Smart routing system performance:\")\n",
    "print(f\"   ‚Ä¢ Total connections: {total_connections}\")\n",
    "print(f\"   ‚Ä¢ Premium QoS ratio: {premium_count}/{total_connections} ({system_efficiency*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Average routing score: {avg_routing_score:.3f}\")\n",
    "print(f\"   ‚Ä¢ Congestion prediction accuracy: {congestion_forecast['congestion_risk']*100:.1f}% risk assessment based on historical patterns\")\n",
    "print(f\"   ‚Ä¢ Physical layer integration: {best_topology} network, {physical_performance['coherence_time']:.1f}s coherence time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem 3: Network-Scale Security Certification\n",
    "\n",
    "Device-independent security protocols require Bell inequality violations to certify quantum correlations without trusting individual hardware components. This analysis implements network-wide certification using adaptive CHSH/Mermin test selection and integrated trust metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.1: European Quantum Network Security Architecture\n",
    "\n",
    "print(\"üîê Problem 3: End-to-End Quantum Network Security Certification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize network security certification system\n",
    "security_system = QuantumNetworkSecurity()\n",
    "\n",
    "# Configure high-priority nodes based on Problem 2 results\n",
    "print(\"üèóÔ∏è  Configuring security nodes based on smart routing results...\")\n",
    "\n",
    "# Define hardware capabilities of European quantum nodes\n",
    "node_capabilities = {\n",
    "    \"Amsterdam\": {\"qubits\": 4, \"fidelity\": 0.97, \"bell_tests\": [\"CHSH\", \"Mermin\"]},\n",
    "    \"Brussels\": {\"qubits\": 3, \"fidelity\": 0.95, \"bell_tests\": [\"CHSH\", \"Mermin\"]},\n",
    "    \"Paris\": {\"qubits\": 2, \"fidelity\": 0.93, \"bell_tests\": [\"CHSH\"]},\n",
    "    \"Berlin\": {\"qubits\": 3, \"fidelity\": 0.94, \"bell_tests\": [\"CHSH\", \"Mermin\"]}\n",
    "}\n",
    "\n",
    "# Add nodes to security system\n",
    "for node_id, capabilities in node_capabilities.items():\n",
    "    security_system.add_node(node_id, capabilities)\n",
    "    print(f\"   ‚úÖ {node_id}: {capabilities['qubits']} qubits, fidelity={capabilities['fidelity']}\")\n",
    "\n",
    "# Establish connections based on Problem 2 routing decisions\n",
    "print(f\"\\\\nüîó Establishing security connections based on Problem 2 routing results...\")\n",
    "\n",
    "# Find Premium and Standard QoS connections\n",
    "high_priority_pairs = []\n",
    "for pair_key, decision in routing_decisions.items():\n",
    "    if decision['qos_level'] in ['premium', 'standard']:\n",
    "        cities = pair_key.split('-')\n",
    "        security_system.add_connection(cities[0], cities[1])\n",
    "        high_priority_pairs.append(pair_key)\n",
    "        print(f\"   üîí {pair_key}: QoS={decision['qos_level']}, routing_score={decision['routing_score']:.3f}\")\n",
    "\n",
    "print(f\"\\\\nüìä Security certification scope:\")\n",
    "print(f\"   Total nodes: {len(node_capabilities)}\")\n",
    "print(f\"   High-priority connections: {len(high_priority_pairs)}\")\n",
    "print(f\"   Connections requiring certification: {len(security_system.connections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.2: Execute Network-Level Bell Inequality Certification\n",
    "\n",
    "print(\"\\\\nüî¨ Executing network-level Bell inequality tests...\")\n",
    "network_cert_results = security_system.run_network_certification()\n",
    "\n",
    "print(\"üîê Network security certification report:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üåê Network certification status: {'‚úÖ CERTIFIED' if network_cert_results['network_certified'] else '‚ùå FAILED'}\")\n",
    "print(f\"üìä Total connections: {network_cert_results['total_connections']}\")\n",
    "print(f\"‚úÖ Passed tests: {network_cert_results['passed_connections']}\")\n",
    "print(f\"‚ùå Failed tests: {network_cert_results['total_connections'] - network_cert_results['passed_connections']}\")\n",
    "print(f\"üìà Pass rate: {network_cert_results['pass_rate']*100:.1f}%\")\n",
    "print(f\"üõ°Ô∏è  Average security parameter: {network_cert_results['average_security_parameter']:.4f}\")\n",
    "print(f\"üîí Minimum security parameter: {network_cert_results['min_security_parameter']:.4f}\")\n",
    "print(f\"üèÜ Network trust score: {network_cert_results['network_trust_score']:.4f}\")\n",
    "\n",
    "# Detailed test results analysis\n",
    "print(f\"\\\\nüìã Individual connection test results:\")\n",
    "test_details = []\n",
    "for (node1, node2), result in network_cert_results['individual_results'].items():\n",
    "    test_details.append({\n",
    "        'Connection': f\"{node1}-{node2}\",\n",
    "        'Test Type': result['test_type'],\n",
    "        'Measured Value': result['measured_value'],\n",
    "        'Classical Bound': result['classical_bound'],\n",
    "        'Violates Classical': '‚úÖ' if result['violates_classical'] else '‚ùå',\n",
    "        'Security Parameter': result['security_parameter']\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame(test_details)\n",
    "print(test_df.round(4))\n",
    "\n",
    "# Security level classification\n",
    "security_levels = []\n",
    "for details in test_details:\n",
    "    security_param = details['Security Parameter']\n",
    "    if security_param >= 0.5:\n",
    "        level = \"High Security\"\n",
    "    elif security_param >= 0.2:\n",
    "        level = \"Medium Security\" \n",
    "    elif security_param > 0:\n",
    "        level = \"Low Security\"\n",
    "    else:\n",
    "        level = \"No Security\"\n",
    "    security_levels.append(level)\n",
    "\n",
    "security_counts = pd.Series(security_levels).value_counts()\n",
    "print(f\"\\\\nüîí Security level distribution:\")\n",
    "for level, count in security_counts.items():\n",
    "    print(f\"   {level}: {count} connections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.3: Integrated Analysis - Three-Layer Security Architecture\n",
    "\n",
    "print(\"\\\\nüéØ Three-layer integrated security analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Integrate results from all three problems\n",
    "integration_summary = {}\n",
    "\n",
    "for pair_key in high_priority_pairs:\n",
    "    cities = pair_key.split('-')\n",
    "    connection_tuple = tuple(cities)\n",
    "    \n",
    "    # Layer 1: Physical layer performance (from Problem 1)\n",
    "    physical_score = physical_performance['final_avg_fidelity']\n",
    "    \n",
    "    # Layer 2: Routing layer performance (from Problem 2)\n",
    "    routing_info = routing_decisions[pair_key]\n",
    "    routing_score = routing_info['routing_score']\n",
    "    qos_level = routing_info['qos_level']\n",
    "    \n",
    "    # Layer 3: Security layer performance (from Problem 3)\n",
    "    if connection_tuple in network_cert_results['individual_results']:\n",
    "        security_result = network_cert_results['individual_results'][connection_tuple]\n",
    "        security_score = security_result['security_parameter']\n",
    "        bell_violation = security_result['violates_classical']\n",
    "    elif tuple(reversed(connection_tuple)) in network_cert_results['individual_results']:\n",
    "        security_result = network_cert_results['individual_results'][tuple(reversed(connection_tuple))]\n",
    "        security_score = security_result['security_parameter']\n",
    "        bell_violation = security_result['violates_classical']\n",
    "    else:\n",
    "        security_score = 0.0\n",
    "        bell_violation = False\n",
    "    \n",
    "    # Calculate integrated security score\n",
    "    integrated_score = (physical_score * 0.3 + routing_score * 0.3 + security_score * 0.4)\n",
    "    \n",
    "    # Security level determination\n",
    "    if integrated_score >= 0.7 and bell_violation:\n",
    "        security_rating = \"üü¢ Enterprise\"\n",
    "    elif integrated_score >= 0.5 and bell_violation:\n",
    "        security_rating = \"üü° Standard\"\n",
    "    elif integrated_score >= 0.3:\n",
    "        security_rating = \"üü† Basic\"\n",
    "    else:\n",
    "        security_rating = \"üî¥ Insecure\"\n",
    "    \n",
    "    integration_summary[pair_key] = {\n",
    "        'physical_score': physical_score,\n",
    "        'routing_score': routing_score,\n",
    "        'security_score': security_score,\n",
    "        'integrated_score': integrated_score,\n",
    "        'qos_level': qos_level,\n",
    "        'bell_violation': bell_violation,\n",
    "        'security_rating': security_rating\n",
    "    }\n",
    "\n",
    "# Build integrated report table\n",
    "integration_data = []\n",
    "for pair_key, summary in integration_summary.items():\n",
    "    integration_data.append({\n",
    "        'Connection': pair_key,\n",
    "        'Physical Score': summary['physical_score'],\n",
    "        'Routing Score': summary['routing_score'], \n",
    "        'Security Score': summary['security_score'],\n",
    "        'Integrated Score': summary['integrated_score'],\n",
    "        'QoS Level': summary['qos_level'],\n",
    "        'Bell Violation': '‚úÖ' if summary['bell_violation'] else '‚ùå',\n",
    "        'Security Rating': summary['security_rating']\n",
    "    })\n",
    "\n",
    "integration_df = pd.DataFrame(integration_data)\n",
    "integration_df = integration_df.sort_values('Integrated Score', ascending=False)\n",
    "\n",
    "print(\"üìä Three-layer integrated security report:\")\n",
    "print(integration_df.round(3))\n",
    "\n",
    "# Overall network security evaluation\n",
    "total_connections = len(integration_summary)\n",
    "enterprise_count = sum(1 for s in integration_summary.values() if s['security_rating'] == \"üü¢ Enterprise\")\n",
    "standard_count = sum(1 for s in integration_summary.values() if s['security_rating'] == \"üü° Standard\")\n",
    "basic_count = sum(1 for s in integration_summary.values() if s['security_rating'] == \"üü† Basic\")\n",
    "insecure_count = sum(1 for s in integration_summary.values() if s['security_rating'] == \"üî¥ Insecure\")\n",
    "\n",
    "print(f\"\\\\nüèÜ European quantum network overall security evaluation:\")\n",
    "print(f\"   Enterprise-level connections: {enterprise_count}/{total_connections} ({enterprise_count/total_connections*100:.1f}%)\")\n",
    "print(f\"   Standard-level connections: {standard_count}/{total_connections} ({standard_count/total_connections*100:.1f}%)\")\n",
    "print(f\"   Basic-level connections: {basic_count}/{total_connections} ({basic_count/total_connections*100:.1f}%)\")\n",
    "print(f\"   Insecure connections: {insecure_count}/{total_connections} ({insecure_count/total_connections*100:.1f}%)\")\n",
    "\n",
    "# Overall network trustworthiness\n",
    "network_trustworthiness = (enterprise_count * 1.0 + standard_count * 0.8 + basic_count * 0.4) / total_connections\n",
    "print(f\"   \\\\nüõ°Ô∏è  Overall network trustworthiness: {network_trustworthiness*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.4: Security Certification Results Visualization and Summary\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Bell test results comparison\n",
    "test_types = [details['Test Type'] for details in test_details]\n",
    "measured_values = [details['Measured Value'] for details in test_details]\n",
    "classical_bounds = [details['Classical Bound'] for details in test_details]\n",
    "connections = [details['Connection'] for details in test_details]\n",
    "\n",
    "x_pos = np.arange(len(connections))\n",
    "bars1 = ax1.bar(x_pos - 0.2, measured_values, 0.4, label='Measured Value', alpha=0.8, color='blue')\n",
    "bars2 = ax1.bar(x_pos + 0.2, classical_bounds, 0.4, label='Classical Bound', alpha=0.8, color='red')\n",
    "\n",
    "ax1.set_xlabel('Quantum Connections')\n",
    "ax1.set_ylabel('Bell Inequality Value')\n",
    "ax1.set_title('Bell Test Results - Quantum vs Classical Bounds')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([conn.replace('-', '\\\\n') for conn in connections], rotation=0)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add violation markers\n",
    "for i, (measured, classical) in enumerate(zip(measured_values, classical_bounds)):\n",
    "    if measured > classical:\n",
    "        ax1.text(i, measured + 0.05, '‚úÖ', ha='center', va='bottom', fontsize=12, color='green')\n",
    "\n",
    "# 2. Three-layer score radar chart (using first connection as example)\n",
    "if integration_data:\n",
    "    example_connection = integration_data[0]\n",
    "    categories = ['Physical Layer', 'Routing Layer', 'Security Layer']\n",
    "    values = [example_connection['Physical Score'], \n",
    "             example_connection['Routing Score'], \n",
    "             example_connection['Security Score']]\n",
    "    \n",
    "    # Prepare radar chart data\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    values += values[:1]  # Close the shape\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax2 = plt.subplot(2, 2, 2, projection='polar')\n",
    "    ax2.plot(angles, values, 'o-', linewidth=2, label=example_connection['Connection'])\n",
    "    ax2.fill(angles, values, alpha=0.25)\n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(categories)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_title(f\"Three-Layer Security Analysis\\\\n({example_connection['Connection']})\", y=1.08)\n",
    "    ax2.grid(True)\n",
    "\n",
    "# 3. Security level distribution\n",
    "security_ratings = [summary['security_rating'] for summary in integration_summary.values()]\n",
    "rating_counts = pd.Series(security_ratings).value_counts()\n",
    "\n",
    "colors_security = {\n",
    "    'üü¢ Enterprise': 'green',\n",
    "    'üü° Standard': 'gold', \n",
    "    'üü† Basic': 'orange',\n",
    "    'üî¥ Insecure': 'red'\n",
    "}\n",
    "\n",
    "bars = ax3.bar(range(len(rating_counts)), rating_counts.values,\n",
    "               color=[colors_security.get(rating, 'gray') for rating in rating_counts.index])\n",
    "\n",
    "ax3.set_xlabel('Security Level')\n",
    "ax3.set_ylabel('Number of Connections')\n",
    "ax3.set_title('Network Security Level Distribution')\n",
    "ax3.set_xticks(range(len(rating_counts)))\n",
    "ax3.set_xticklabels([rating.split()[1] for rating in rating_counts.index])\n",
    "\n",
    "# Add count labels\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "             f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "# 4. Integrated score scatter plot\n",
    "integrated_scores = [summary['integrated_score'] for summary in integration_summary.values()]\n",
    "security_scores = [summary['security_score'] for summary in integration_summary.values()]\n",
    "connection_names = list(integration_summary.keys())\n",
    "\n",
    "scatter = ax4.scatter(integrated_scores, security_scores, \n",
    "                     c=[colors_security.get(rating, 'gray') for rating in security_ratings],\n",
    "                     s=100, alpha=0.8)\n",
    "\n",
    "ax4.set_xlabel('Integrated Security Score')\n",
    "ax4.set_ylabel('Bell Test Security Parameter')\n",
    "ax4.set_title('Security Score Correlation Analysis')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add connection labels\n",
    "for i, name in enumerate(connection_names):\n",
    "    ax4.annotate(name.replace('-', '\\\\n'), \n",
    "                (integrated_scores[i], security_scores[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ Problem 3 completed! End-to-end security certification summary:\")\n",
    "print(f\"   ‚Ä¢ Overall network certification: {'‚úÖ PASSED' if network_cert_results['network_certified'] else '‚ùå FAILED'}\")\n",
    "print(f\"   ‚Ä¢ Bell test pass rate: {network_cert_results['pass_rate']*100:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Average security parameter: {network_cert_results['average_security_parameter']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Network trust score: {network_cert_results['network_trust_score']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Overall trustworthiness: {network_trustworthiness*100:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Enterprise-level connection ratio: {enterprise_count}/{total_connections}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Analysis Summary\n",
    "\n",
    "## Problem Solutions\n",
    "\n",
    "### Topology Optimization\n",
    "- **Physical modeling**: Spatial correlation effects on network-wide decoherence\n",
    "- **Performance comparison**: Linear vs star topology analysis with realistic parameters  \n",
    "- **Optimization metrics**: Coherence time and fidelity preservation\n",
    "\n",
    "### Adaptive Routing\n",
    "- **Traffic modeling**: Time-varying demand patterns with application-specific characteristics\n",
    "- **Decision algorithms**: Multi-layer routing score integration (physical + traffic + stability)\n",
    "- **QoS classification**: Three-tier service differentiation with congestion prediction\n",
    "\n",
    "### Security Certification  \n",
    "- **Bell test protocols**: Adaptive CHSH/Mermin selection based on system capabilities\n",
    "- **Network coordination**: Multi-node test scheduling and result aggregation\n",
    "- **Trust quantification**: Cross-layer security score integration\n",
    "\n",
    "## Technical Contributions\n",
    "\n",
    "### Cross-Layer Integration\n",
    "- Physical layer constraints influence routing decisions\n",
    "- Traffic patterns inform resource allocation strategies  \n",
    "- Security requirements drive topology selection\n",
    "\n",
    "### Scalability Analysis\n",
    "- Coherence time limitations constrain decision windows\n",
    "- Distributed decoherence modeling for realistic networks\n",
    "- Network-wide performance optimization under quantum constraints\n",
    "\n",
    "### Implementation Framework\n",
    "- Modular tool architecture for problem decomposition\n",
    "- Quantitative metrics for performance evaluation\n",
    "- Extensible design for larger network analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Future Extensions\n",
    "\n",
    "- **NetSquid integration**: Validation using established quantum network simulators\n",
    "- **Multi-layer repeaters**: Entanglement purification and multiplexing protocols\n",
    "- **ML-enhanced routing**: Deep learning for traffic prediction and path optimization\n",
    "- **Large-scale networks**: Analysis scaling to metropolitan and continental networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
